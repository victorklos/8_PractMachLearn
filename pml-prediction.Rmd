---
title: "PML Prediction"
author: "Victor Klos"
date: "13-03-2015"
output: html_document
---

TODO introduction 


## Loading and cleaning the data

The data is already split into a training and testing set. The format is `csv` which is readily handled by R (even though the extraneous quotes require a bit of a detour). Running a `summary` on the training set reveals that many columns contain the phrase `#DIV/0!`, so the data suffers from an export problem. Also, the column names are not 'tidy' as they contain underscores.

Knowing this, loading and tidying becomes:

```{r}
nas <- c("", "\"\"", "NA", "#DIV/0!")
training <- read.csv("pml-training.csv", colClasses="character", na.strings=nas)
testing <- read.csv("pml-testing.csv", colClasses="character", na.strings=nas)
colnames(training) <- colnames(testing) <- tolower(gsub("_", ".", colnames(training)))

classes <- c("integer", rep("factor", 5), rep("numeric", 153), "factor")
for (i in 1:ncol(training)) {
  training[,i] <- do.call(paste("as", classes[i], sep="."), list(training[,i]))
  testing[,i] <- do.call(paste("as", classes[i], sep="."), list(testing[,i]))
}
testing <- testing[-ncol(testing)]
```

The training data set contains `r dim(training)[2]` columns. The testing data set has one less; it is the 'classe' column that needs to be predicted. The training set contains `r dim(training)[1]` rows and the testing set has `r dim(testing)[1]` rows.


## Exploratory analysis

From looking at the summary it was obvious that many columns in the training dataset suffer from missing values. Let's examine the extent of this potential problem:

```{r}
nas.per.col <- apply(training, 2, FUN=function(col) sum(is.na(col)))
table(nas.per.col)
```

This overview tells us that _(i)_ NA's are abundant and _(ii)_ there is no fixing them as there exist no columns with only a few missing values. Hence, all columns with NA's need to be removed from the dataset.

Additionally, the rows with the index, user name, time stamps and window information will not aid in building a good predictor so these are removed too:

```{r}
cols.to.remove <- c(1:7, which(nas.per.col > 0))
training <- training[-cols.to.remove]
testing <- testing[-cols.to.remove]
```

Next, we can check if the remaining data isn't near zero or without variance:

```{r, message=F}
library(caret)
nz <- nearZeroVar(training, saveMetrics=T)
sum(nz$zeroVar) + sum(nz$nzv)
```

The outcome of zero indicates that all data may have relevance for the predictor.


## Building predictors

For our purposes, the training dataset is subsetted into a training dataset and a probe dataset. The latter will be used to estimate the out-of-sample error of our predictors:

```{r}
#training <- training[sample(nrow(training), nrow(training)*.2),]
in.train <- createDataPartition(y=training$classe, p=.7, list=F)
probe <- training[-in.train,]
training <- training[in.train,]
```

The goal of the assignment is to build a predictor for the `classe` variable:

```{r, echo=F}
summary(training$classe)
```

```{r, echo=F, message=F}
# Set up for parallel processing as per
# https://class.coursera.org/predmachlearn-012/forum/thread?thread_id=61
library(parallel); library(doParallel)
registerDoParallel(makeForkCluster(detectCores()))
```

### Generating prediction models

It is well known that `random forests` (RF) give the best out-of-the-box results when it comes to building predictors from unstructured data. Their results however are not very human-interpretable. For this reason `rpart`, a recursive partitioning (RP) model, is also included. While looking for alternatives to RF a `neural network` (NN) was tried too but with little success (around 50% accuracy).

```{r, message=F, warning=F}
set.seed(42)

fit.rf <- train(classe ~ ., data=training, method="rf")
fit.rp <- train(classe ~ ., data=training, method="rpart2")
fit.nn <- train(classe ~ ., data=training, method="nnet", maxit = 1000,
                tuneGrid = expand.grid(.decay = .04, .size = c(11,25,35)),
                trace=F, linout=T)
```

### Results

```{r}
cm.rf <- confusionMatrix(predict(fit.rf, newdata=probe), probe$classe)
cm.rp <- confusionMatrix(predict(fit.rp, newdata=probe), probe$classe)
cm.nn <- confusionMatrix(predict(fit.nn, newdata=probe), probe$classe)
cbind(as.data.frame.matrix(cm.rf$table), o=rep("",5),
      as.data.frame.matrix(cm.rp$table), o=rep("",5),
      as.data.frame.matrix(cm.nn$table))
```

The table above includes the confusion matrices of the Random Forest model, the Recursive Partinioning model and the Neural Network model (from left to right) with the prediction vertically against the reference horizontally.

The model accuracies in numbers:

```{r}
rbind(RF=cm.rf$overall, RP=cm.rp$overall, NN=cm.nn$overall)[,c(2,3,4,1)]
```

Note that because the predictions were done on the probe set, the accuries in this section reflect the expected out-of-sample behaviour.

As promised, here is a decision tree for some human insight:

```{r, message=F, dev='svg'}
library(rattle)
fancyRpartPlot(fit.rp$finalModel, sub="")
```


### Model selection

It is quite clear from the accuracy tables in the previous section that RF outperforms the others by quite some length. Accordingly, this is the selected model.

## Testing

Finally let's test our predictor against the testing data set:

```{r}
print(outcome <- predict(fit.rf, testing))
```

