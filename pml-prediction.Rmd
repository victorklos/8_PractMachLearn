---
title: "PML Prediction"
author: "Victor Klos"
date: "17-03-2015"
output: html_document
---

This writeup is for the assignment part of the [Practical Machine Learning](https://class.coursera.org/predmachlearn-012/) course at [Coursera](http://coursera.org). The goal is to create a predictor for the `classe` variable of a given dataset. This variable indicates how well some excersise is performed by a group of test subjects.

More details on the assignment can be found at the [assignment page](https://class.coursera.org/predmachlearn-012/human_grading/view/courses/973547/assessments/4/submissions), more details on the data set are available at the [Human Activity Recognition project page](http://groupware.les.inf.puc-rio.br/har).


## Loading and cleaning the data

The data is already split into a training and testing set. The format is `csv` which is readily handled by R (even though the extraneous quotes require a bit of a detour). Running a `summary` on the training set reveals that many columns contain the phrase `#DIV/0!`, so the data suffers from an export problem. Also, the column names are not 'tidy' as they contain underscores.

Knowing this, loading and tidying becomes:

```{r}
nas <- c("", "\"\"", "NA", "#DIV/0!")
training <- read.csv("pml-training.csv", colClasses="character", na.strings=nas)
testing <- read.csv("pml-testing.csv", colClasses="character", na.strings=nas)
colnames(training) <- colnames(testing) <- tolower(gsub("_", ".", colnames(training)))

classes <- c("integer", rep("factor", 5), rep("numeric", 153), "factor")
for (i in 1:ncol(training)) {
  training[,i] <- do.call(paste("as", classes[i], sep="."), list(training[,i]))
  testing[,i] <- do.call(paste("as", classes[i], sep="."), list(testing[,i]))
}
testing <- testing[-ncol(testing)] # remove problem_id column
```

The training data set contains `r dim(training)[2]` columns (or 'features'). The testing data set has one less; it is the 'classe' column that needs to be predicted. The training set contains `r dim(training)[1]` rows and the testing set has `r dim(testing)[1]` rows.

From looking at the summary it was obvious that many columns in the training dataset suffer from missing values. Let's examine the extent of this potential problem:

```{r}
nas.per.col <- apply(training, 2, FUN=function(col) sum(is.na(col)))
table(nas.per.col)
```

This overview tells us that _(i)_ NA's are abundant and _(ii)_ there is no fixing them as there exist no columns with only a few missing values. Hence, all columns with NA's need to be removed from the dataset.

Additionally, the rows with the index, user name, time stamps and window information will not aid in building a good predictor so these are removed too:

```{r}
cols.to.remove <- c(1:7, which(nas.per.col > 0))
training <- training[-cols.to.remove]
testing <- testing[-cols.to.remove]
```


## Exploratory analysis

Our dataset has been reduced to `r dim(testing)[2]` columns. It is important to know if these features aren't near zero or without variance:

```{r, message=F}
library(caret)
nz <- nearZeroVar(training, saveMetrics=T)
sum(nz$zeroVar) + sum(nz$nzv)
```

The outcome of zero indicates that all featues may have relevance for a predictor.

The goal of the assignment is to build a predictor for the `classe` variable, which is distributed quite evenly:

```{r, echo=F}
summary(training$classe)
```

As the remaining features are quite large in number and many "are just numbers" (and difficult to interpret by humans) we will skip further exploration for now and see how well we can do without. 


## Building predictors

For our purposes, the training dataset is subsetted into a training dataset and a probe dataset. The latter will be used to estimate the out-of-sample error of our predictors:

```{r}
#training <- training[sample(nrow(training), nrow(training)*.2),]
inTrain <- createDataPartition(y=training$classe, p=.7, list=F)
probe <- training[-inTrain,]
training <- training[inTrain,]
```

```{r, echo=F, message=F}
# Set up for parallel processing as per
# https://class.coursera.org/predmachlearn-012/forum/thread?thread_id=61
library(parallel); library(doParallel)
registerDoParallel(makeForkCluster(detectCores()))
```

### Generating prediction models

It is well known that `random forests` (RF) give excellent out-of-the-box results when it comes to building predictors from unstructured data. Their results however are not very human-interpretable. For this reason `rpart`, a recursive partitioning (RP) model, is also included. While searching for alternatives to RF a `neural network` (NN) was found and included, out of curiosity.

```{r, message=F, warning=F}
set.seed(42)
tc <- trainControl(method = "repeatedcv", repeats = 3, number=5)
fit.rf <- train(classe ~ ., data=training, trControl=tc, method="rf")
fit.rp <- train(classe ~ ., data=training, trControl=tc, method="rpart2")
fit.nn <- train(classe ~ ., data=training, trControl=tc, method="nnet", maxit = 1000,
                tuneGrid = expand.grid(.decay = .04, .size = c(10,20,30)),
                trace=F, linout=T, preProc=c("center", "scale"))
```

Besides cross-validating three different algorithms, each algorithm itself is cross-validated. The `trainControl` parameter above specifies a Repeated Cross Validation with 3 repeats and a 5-fold random subsampling. This should suffice to obtain a well-estimated out-of-sample error from the in-sample training dataset.

### Results

```{r}
cm.rf <- confusionMatrix(predict(fit.rf, newdata=probe), probe$classe)
cm.rp <- confusionMatrix(predict(fit.rp, newdata=probe), probe$classe)
cm.nn <- confusionMatrix(predict(fit.nn, newdata=probe), probe$classe)
cbind(as.data.frame.matrix(cm.rf$table), " "=rep("",5),
      as.data.frame.matrix(cm.rp$table), " "=rep("",5),
      as.data.frame.matrix(cm.nn$table))
```

This table comprises the confusion matrices of the Random Forest model, the Recursive Partitioning model and the Neural Network model (from left to right) with the prediction vertically against the reference horizontally. The differences in accuracy of the three algorithms are clearly visible.

The models accuracies in numbers:

```{r}
rbind(RF=cm.rf$overall, RP=cm.rp$overall, NN=cm.nn$overall)[,c(3,4,1)]
```

Note that because the predictions were done on the probe set, the accuries above reflect the expected out-of-sample behaviour. For completeness, here is the in-sample accuracy prediction:

```{r}
fit.rf$results[,c(1,2,4)]
```

So using Repeated Cross Validation with the above parameters causes a slight _underestimation_ of the final out-of-sample accuracy.

As promised, here is a decision tree for some human insight:

```{r, message=F, dev='svg', fig.width=10}
library(rattle)
fancyRpartPlot(fit.rp$finalModel, sub="")
```

As the accuracy of the RP model is only `r round(100*cm.rp$overall[1],0)`% this figure should not be used as guidance when determining a classification.


### Model selection

From the accuracy tables in the previous section it is evident that RF outperforms the others by quite some length. Accordingly and unsurprisingly, this model is selected.

## Verification

Finally let's run our winning predictor against the testing data set:

```{r, echo=F}
# From the submission page
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
```

```{r}
answers <- predict(fit.rf, testing)
pml_write_files(answers)
#print(answers)
print("Printing supressed as that may violate the Coursera Honor Code.")
```


## Conclusions and remarks

After cleaning the data and removing many columns with NA's, a tidy data set was gained from the input files. This set was split 70-30 into a training set and a probe set. With the training set, three machine learning algorithms were instructed to train a model:

1. Random Forests
2. Recursive Partitioning
3. Neural Network

From these algorithms the first one (RF) yielded the best result with an out-of-sample accuracy of over 99%. The predictions with the testing set scored 20 out of 20 (100%).

Interestingly enough, the Neural Network (NN) came in second. During the development of this report many runs were performed with a reduced size training set in order to reduce waiting times. During those runs, the NN didn't always impress with accuracy  scores below 50%. This goes to show that (dataset) size does matter :-).

Finally it may be worth mentioning that this writeup is the result of redoing the complete Practical Machine Learning class on Coursera and quite some interweb and forum searching. Many thanks to all the helpful people out there!
