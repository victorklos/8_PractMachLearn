---
title: "PML Prediction"
author: "Victor Klos"
date: "13-03-2015"
output: html_document
---

TODO introduction 


## Loading and cleaning the data

The data is already split into a training and testing set. The format is `csv` which is readily handled by R (even though the extraneous quotes require a bit of a detour). Running a `summary` on the training set reveals that many columns contain the phrase `#DIV/0!`, so the data suffers from an export problem. Also, the column names are not 'tidy' as they contain underscores.

Knowing this, loading and tidying becomes:

```{r}
nas <- c("", "\"\"", "NA", "#DIV/0!")
training <- read.csv("pml-training.csv", colClasses="character", na.strings=nas)
testing <- read.csv("pml-testing.csv", colClasses="character", na.strings=nas)
colnames(training) <- colnames(testing) <- tolower(gsub("_", ".", colnames(training)))

classes <- c("integer", rep("factor", 5), rep("numeric", 153), "factor")
for (i in 1:ncol(training)) {
  training[,i] <- do.call(paste("as", classes[i], sep="."), list(training[,i]))
  testing[,i] <- do.call(paste("as", classes[i], sep="."), list(testing[,i]))
}
testing <- testing[-ncol(testing)] # remove problem_id column
```

The training data set contains `r dim(training)[2]` columns. The testing data set has one less; it is the 'classe' column that needs to be predicted. The training set contains `r dim(training)[1]` rows and the testing set has `r dim(testing)[1]` rows.


## Exploratory analysis

From looking at the summary it was obvious that many columns in the training dataset suffer from missing values. Let's examine the extent of this potential problem:

```{r}
nas.per.col <- apply(training, 2, FUN=function(col) sum(is.na(col)))
table(nas.per.col)
```

This overview tells us that _(i)_ NA's are abundant and _(ii)_ there is no fixing them as there exist no columns with only a few missing values. Hence, all columns with NA's need to be removed from the dataset.

Additionally, the rows with the index, user name, time stamps and window information will not aid in building a good predictor so these are removed too:

```{r}
cols.to.remove <- c(1:7, which(nas.per.col > 0))
training <- training[-cols.to.remove]
testing <- testing[-cols.to.remove]
```

Next, we can check if the remaining data isn't near zero or without variance:

```{r, message=F}
library(caret)
nz <- nearZeroVar(training, saveMetrics=T)
sum(nz$zeroVar) + sum(nz$nzv)
```

The outcome of zero indicates that all data may have relevance for the predictor.


## Building predictors

For our purposes, the training dataset is subsetted into a training dataset and a probe dataset. The latter will be used to estimate the out-of-sample error of our predictors:

```{r}
#training <- training[sample(nrow(training), nrow(training)*.25),]
in.train <- createDataPartition(y=training$classe, p=.7, list=F)
probe <- training[-in.train,]
training <- training[in.train,]
```

The goal of the assignment is to build a predictor for the `classe` variable, which is distributed as follows (in the training set):

```{r, echo=F}
summary(training$classe)
```

```{r, echo=F, message=F}
# Set up for parallel processing as per
# https://class.coursera.org/predmachlearn-012/forum/thread?thread_id=61
library(parallel); library(doParallel)
registerDoParallel(makeForkCluster(detectCores()))
```

### Generating prediction models

It is well known that `random forests` (RF) give the best out-of-the-box results when it comes to building predictors from unstructured data. Their results however are not very human-interpretable. For this reason `rpart`, a recursive partitioning (RP) model, is also included. While looking for alternatives to RF a `neural network` (NN) was tried too, out of curiosity.

```{r, message=F, warning=F}
set.seed(42)
tc <- trainControl(method = "repeatedcv", repeats = 3, number=5)
fit.rf <- train(classe ~ ., data=training, trControl=tc, method="rf")
fit.rp <- train(classe ~ ., data=training, trControl=tc, method="rpart2")
fit.nn <- train(classe ~ ., data=training, trControl=tc, method="nnet", maxit = 1000,
                tuneGrid = expand.grid(.decay = .04, .size = c(15,25,35)),
                trace=F, linout=T, preProc=c("center", "scale"))
```

Besides cross-validating three different algorithms, each algorithm itself is cross-validated. The `trainControl` parameter above specifies a Repeated Cross Validation with 3 repeats and a 5-fold random subsampling. This should suffice to obtain a well-estimated out-of-sample error from the in-sample training dataset.

### Results

```{r}
cm.rf <- confusionMatrix(predict(fit.rf, newdata=probe), probe$classe)
cm.rp <- confusionMatrix(predict(fit.rp, newdata=probe), probe$classe)
cm.nn <- confusionMatrix(predict(fit.nn, newdata=probe), probe$classe)
cbind(as.data.frame.matrix(cm.rf$table), "   "=rep("",5),
      as.data.frame.matrix(cm.rp$table), "   "=rep("",5),
      as.data.frame.matrix(cm.nn$table))
```

The table above includes the confusion matrices of the Random Forest model, the Recursive Partinioning model and the Neural Network model (from left to right) with the prediction vertically against the reference horizontally.

The model accuracies in numbers:

```{r}
rbind(RF=cm.rf$overall, RP=cm.rp$overall, NN=cm.nn$overall)[,c(3,4,1)]
```

Note that because the predictions were done on the probe set, the accuries above reflect the expected out-of-sample behaviour. For completeness, here is the in-sample accuracy prediction:

```{r}
fit.rf$results[,c(1,2,4)]
```

So using Repeated Cross Validation with the above parameters causes a slight _underestimation_ of the final out-of-sample accuracy.

As promised, here is a decision tree for some human insight:

```{r, message=F, dev='svg', fig.width=10}
library(rattle)
fancyRpartPlot(fit.rp$finalModel, sub="")
```

As the accuracy of the RP model is only `r round(100*cm.rp$overall[1],0)`% this figure should not be used as guidance when determining a classification.


### Model selection

From the accuracy tables in the previous section it is evident that RF outperforms the others by quite some length. Accordingly and unsurprisingly, this is the selected model.

## Verification

Finally let's run our predictor against the testing data set:

```{r, echo=F}
# From the submission page
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
```

```{r}
answers <- predict(fit.rf, testing)
pml_write_files(answers)
#print(answers)
print("Printing supressed as that may violate the Coursera Honor Code.")
```


## Conclusions and remarks

After cleaning the data and removing many columns with NA's, a tidy data set was gained from the input files. This set was split 70-30 into a training set and a probe set. With the training set, three machine learning algorithms were instructed to create a model:

1. Random Forests
2. Recursive Partitioning
3. Neural Network

From these algorithms the first one (RF) yielded the best result with an out-of-sample accuracy of over 99%. The predictions with the testing set scored 20 out of 20 (100%).

Interestingly enough, the Neural Network (NN) came in second. During the development of this report many runs were performed with a reduced size training set in order to reduce waiting times. During those runs, the NN didn't always impress with accuracy  scores below 50%. This goes to show that (dataset) size does matter :-).

Finally it may be worth mentioning that this writeup is the result of redoing the complete Practical Machine Learning class on Coursera and quite some interweb and forum searching. Many thanks to all the helpful people out there!
